{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install transformers\n",
    "!pip install keras\n",
    "!pip install Sentencepiece\n",
    "!pip install torch torchvision torchaudio --extra-index-url https://download.pytorch.org/whl/cu113\n",
    "!pip install datasets\n",
    "!pip install -U scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "#os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"\n",
    "#os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n",
    "os.environ[\"WANDB_DISABLED\"] = \"true\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-15 12:02:15.272779: I tensorflow/core/util/util.cc:169] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n"
     ]
    }
   ],
   "source": [
    "import transformers\n",
    "import torch\n",
    "#import keras\n",
    "from datasets import load_dataset, concatenate_datasets, Dataset\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from functions import *\n",
    "\n",
    "from transformers import TextClassificationPipeline\n",
    "from transformers.pipelines.pt_utils import KeyDataset\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, precision_recall_fscore_support\n",
    "from transformers import AutoModelForSequenceClassification, TrainingArguments, Trainer, AutoTokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Training of Baseline Classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#features = [\"valence\"]\n",
    "label_num = 4\n",
    "#conf_scores = [0.5, 0.7, 0.95]\n",
    "conf_score = 0.5\n",
    "#load_tokenizer(\"distilbert-base-german-cased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default-3edc0ad1ac61e499\n",
      "Found cached dataset csv (/home/jovyan/.cache/huggingface/datasets/csv/default-3edc0ad1ac61e499/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "460257b5a2f647afb5bf36d9ef595c04",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default-4bcdf5d906130417\n",
      "Found cached dataset csv (/home/jovyan/.cache/huggingface/datasets/csv/default-4bcdf5d906130417/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "951c808d4ee043c9acdbfd614b95baa8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n"
     ]
    }
   ],
   "source": [
    "train = load_tokenize_enhanced_set(\"../Ablation_Studies/forum/sublabel/train_sublabel_preproc.csv\")\n",
    "test = load_tokenize_enhanced_set(\"../Ablation_Studies/forum/sublabel/test_sublabel_preproc.csv\")\n",
    "training_args = TrainingArguments(\"original_forum\", evaluation_strategy=\"epoch\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file config.json from cache at /home/jovyan/.cache/huggingface/hub/models--bert-base-german-cased/snapshots/702774c02b32a4f360d5fea60ab034d64bf0141c/config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"bert-base-german-cased\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\",\n",
      "    \"3\": \"LABEL_3\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2,\n",
      "    \"LABEL_3\": 3\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.24.0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30000\n",
      "}\n",
      "\n",
      "loading weights file pytorch_model.bin from cache at /home/jovyan/.cache/huggingface/hub/models--bert-base-german-cased/snapshots/702774c02b32a4f360d5fea60ab034d64bf0141c/pytorch_model.bin\n",
      "Some weights of the model checkpoint at bert-base-german-cased were not used when initializing BertForSequenceClassification: ['cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-german-cased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForSequenceClassification.from_pretrained(\"bert-base-german-cased\", num_labels=label_num)\n",
    "trainer = Trainer(model=model, args=training_args, train_dataset=train, eval_dataset=test, compute_metrics=compute_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the training set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: function_Sentence, nonfunction_Sentence, non_noun_verb_adj, Sentence, non_noun, non_verb, non_verb_aux, non_noun_adj. If function_Sentence, nonfunction_Sentence, non_noun_verb_adj, Sentence, non_noun, non_verb, non_verb_aux, non_noun_adj are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "/opt/conda/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 2411\n",
      "  Num Epochs = 3\n",
      "  Instantaneous batch size per device = 8\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 16\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 453\n",
      "  Number of trainable parameters = 109084420\n",
      "/opt/conda/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='453' max='453' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [453/453 02:31, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>Mcc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.546480</td>\n",
       "      <td>0.788591</td>\n",
       "      <td>0.733440</td>\n",
       "      <td>0.696260</td>\n",
       "      <td>0.783548</td>\n",
       "      <td>0.597685</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.519716</td>\n",
       "      <td>0.818792</td>\n",
       "      <td>0.738956</td>\n",
       "      <td>0.742000</td>\n",
       "      <td>0.737157</td>\n",
       "      <td>0.623796</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.654404</td>\n",
       "      <td>0.830537</td>\n",
       "      <td>0.761279</td>\n",
       "      <td>0.755587</td>\n",
       "      <td>0.768865</td>\n",
       "      <td>0.650737</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: function_Sentence, nonfunction_Sentence, non_noun_verb_adj, Sentence, non_noun, non_verb, non_verb_aux, non_noun_adj. If function_Sentence, nonfunction_Sentence, non_noun_verb_adj, Sentence, non_noun, non_verb, non_verb_aux, non_noun_adj are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 596\n",
      "  Batch size = 16\n",
      "/opt/conda/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: function_Sentence, nonfunction_Sentence, non_noun_verb_adj, Sentence, non_noun, non_verb, non_verb_aux, non_noun_adj. If function_Sentence, nonfunction_Sentence, non_noun_verb_adj, Sentence, non_noun, non_verb, non_verb_aux, non_noun_adj are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 596\n",
      "  Batch size = 16\n",
      "/opt/conda/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: function_Sentence, nonfunction_Sentence, non_noun_verb_adj, Sentence, non_noun, non_verb, non_verb_aux, non_noun_adj. If function_Sentence, nonfunction_Sentence, non_noun_verb_adj, Sentence, non_noun, non_verb, non_verb_aux, non_noun_adj are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 596\n",
      "  Batch size = 16\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Saving model checkpoint to classif/final_sublabel_model\n",
      "Configuration saved in classif/final_sublabel_model/config.json\n",
      "Model weights saved in classif/final_sublabel_model/pytorch_model.bin\n",
      "tokenizer config file saved in classif/final_sublabel_model/tokenizer_config.json\n",
      "Special tokens file saved in classif/final_sublabel_model/special_tokens_map.json\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('classif/final_sublabel_model/tokenizer_config.json',\n",
       " 'classif/final_sublabel_model/special_tokens_map.json',\n",
       " 'classif/final_sublabel_model/vocab.txt',\n",
       " 'classif/final_sublabel_model/added_tokens.json',\n",
       " 'classif/final_sublabel_model/tokenizer.json')"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()\n",
    "trainer.save_model(\"classif/final_sublabel_model\")\n",
    "tokenizer.save_pretrained(\"classif/final_sublabel_model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Finetuning with winning confidence thresholds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "#### Concatenating GLoHBCD-Weight, GLoHBCD-Smoke + Annomi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "weight_loss = pd.read_csv(\"new_data_both.csv\")\n",
    "weight_loss = weight_loss[\"split\"].copy()\n",
    "train_rest = pd.read_csv(\"Nonsmoking_addition/baldNichtraucher_annotiert.csv\", sep=\";\")\n",
    "valence_ns = pd.read_csv(\"Nonsmoking_addition/ns_val_train_set.csv\")\n",
    "valence_ns = valence_ns[\"Sentence\"].copy()\n",
    "annomi = pd.read_csv(\"Nonsmoking_addition/val_AnnoMI_predicted_set.csv\")\n",
    "annomi = annomi[\"Sentence\"].copy()\n",
    "\n",
    "labels = [\"R\", \"C\", \"TS\"]\n",
    "\n",
    "train_rest = train_rest[~train_rest[\"label\"].isin(labels)].copy()\n",
    "train_rest = train_rest[\"sentence\"].copy()\n",
    "\n",
    "df = pd.concat([weight_loss, train_rest, valence_ns, annomi])\n",
    "df.to_csv(\"Valence_all_new_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "weight_loss = pd.read_csv(\"new_data_both.csv\")\n",
    "weight_loss = weight_loss[\"split\"].copy()\n",
    "train_rest = pd.read_csv(\"Nonsmoking_addition/baldNichtraucher_annotiert.csv\", sep=\";\")\n",
    "valence_ns = pd.read_csv(\"Nonsmoking_addition/ns_label_train_set.csv\")\n",
    "valence_ns = valence_ns[\"Sentence\"].copy()\n",
    "annomi = pd.read_csv(\"Nonsmoking_addition/val_AnnoMI_predicted_set.csv\")\n",
    "annomi = annomi[\"Sentence\"].copy()\n",
    "\n",
    "labels = [\"R\", \"C\", \"TS\"]\n",
    "\n",
    "train_rest = train_rest[~train_rest[\"label\"].isin(labels)].copy()\n",
    "train_rest = train_rest[\"sentence\"].copy()\n",
    "\n",
    "df = pd.concat([weight_loss, train_rest, valence_ns, annomi])\n",
    "df.to_csv(\"Label_all_new_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "weight_loss = pd.read_csv(\"new_data_both.csv\")\n",
    "weight_loss = weight_loss[\"split\"].copy()\n",
    "train_rest = pd.read_csv(\"Nonsmoking_addition/baldNichtraucher_annotiert.csv\", sep=\";\")\n",
    "valence_ns = pd.read_csv(\"Nonsmoking_addition/ns_sublabel_train_set.csv\")\n",
    "valence_ns = valence_ns[\"Sentence\"].copy()\n",
    "annomi = pd.read_csv(\"Nonsmoking_addition/val_AnnoMI_predicted_set.csv\")\n",
    "annomi = annomi[\"Sentence\"].copy()\n",
    "\n",
    "labels = [\"R\", \"C\", \"TS\"]\n",
    "\n",
    "train_rest = train_rest[~train_rest[\"label\"].isin(labels)].copy()\n",
    "train_rest = train_rest[\"sentence\"].copy()\n",
    "\n",
    "df = pd.concat([weight_loss, train_rest, valence_ns, annomi])\n",
    "df.to_csv(\"Sublabel_all_new_data.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### creating pseudo labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default-9a84bf4b2af12b2f\n",
      "Found cached dataset csv (/home/jovyan/.cache/huggingface/datasets/csv/default-9a84bf4b2af12b2f/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317)\n"
     ]
    }
   ],
   "source": [
    "# Valence\n",
    "label_num = 2\n",
    "conf_score = 0.5\n",
    "val_FN_winner = load_dataset(\"csv\", data_files=\"Valence_all_new_data.csv\", split=\"train\")\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"classif/final_valence_model\", num_labels=label_num)\n",
    "pipe = TextClassificationPipeline(model=model, tokenizer=tokenizer, return_all_scores=True, device=0)\n",
    "\n",
    "label_0_scores = []\n",
    "label_1_scores = []\n",
    "#for out in pipe(list(X_train_base[i]), batch_size=8, truncation=\"only_first\"):\n",
    "for out in pipe(KeyDataset(val_FN_winner, \"0\"), batch_size=8, truncation=\"only_first\"): \n",
    "    label_0_scores.append(out[0]['score'])\n",
    "    label_1_scores.append(out[1]['score'])\n",
    "pred_train_df = pd.DataFrame({\"Sentence\": val_FN_winner[:][\"0\"], \"0\":label_0_scores, \"1\": label_1_scores})\n",
    "_1 = pred_train_df[pred_train_df[\"0\"] > conf_score].copy()\n",
    "a_1 = pred_train_df[pred_train_df[\"1\"] > conf_score].copy()\n",
    "_1[\"labels\"] = 0\n",
    "a_1[\"labels\"] = 1\n",
    "predict_1 = pd.concat([_1,a_1]).sample(frac=0.2)\n",
    "#predict_1.rename(columns={'split':'Sentence'}, inplace=True)\n",
    "predict_1.to_csv(\"valence_predicted_enhanced_set.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_num = 2\n",
    "enhanced = load_tokenize_enhanced_set(\"valence_predicted_enhanced_set.csv\")\n",
    "orig = load_tokenize_enhanced_set(\"../Ablation_Studies/forum/valence/train_valence_preproc.csv\")\n",
    "train = concatenate_datasets([enhanced, orig]).shuffle()\n",
    "test = load_tokenize_enhanced_set(\"../Ablation_Studies/forum/valence/test_valence_preproc.csv\")\n",
    "\n",
    "training_args = TrainingArguments(\"original_forum\", evaluation_strategy=\"epoch\")\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"classif/final_valence_model\", num_labels=label_num)\n",
    "trainer = Trainer(model=model, args=training_args, train_dataset=train, eval_dataset=test, compute_metrics=compute_metrics)\n",
    "\n",
    "print(\"original:\")\n",
    "print(trainer.evaluate())\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"bert-base-german-cased\", num_labels=label_num)\n",
    "trainer = Trainer(model=model, args=training_args, train_dataset=train, eval_dataset=test, compute_metrics=compute_metrics)\n",
    "trainer.train()\n",
    "\n",
    "print(\"with automatically annotated data:\")\n",
    "print(trainer.evaluate())\n",
    "            \n",
    "trainer.save_model(\"classif/final_valence_fuzzy_model_ooD\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default-e4ed113be21c524c\n",
      "Found cached dataset csv (/home/jovyan/.cache/huggingface/datasets/csv/default-e4ed113be21c524c/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317)\n",
      "loading configuration file classif/final_label_model/config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"classif/final_label_model\",\n",
      "  \"architectures\": [\n",
      "    \"BertForSequenceClassification\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"problem_type\": \"single_label_classification\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.25.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30000\n",
      "}\n",
      "\n",
      "loading weights file classif/final_label_model/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing BertForSequenceClassification.\n",
      "\n",
      "All the weights of BertForSequenceClassification were initialized from the model checkpoint at classif/final_label_model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use BertForSequenceClassification for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "label_num = 3\n",
    "conf_score = 0.95\n",
    "lab_FN_winner = load_dataset(\"csv\", data_files=\"Label_all_new_data.csv\", split=\"train\")\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"classif/final_label_model\", num_labels=label_num)\n",
    "pipe = TextClassificationPipeline(model=model, tokenizer=tokenizer, return_all_scores=True, device=0)\n",
    "\n",
    "label_0_scores = []\n",
    "label_1_scores = []\n",
    "label_2_scores = []\n",
    "for out in pipe(KeyDataset(lab_FN_winner, \"0\"), batch_size=8, truncation=\"only_first\"):\n",
    "    label_0_scores.append(out[0]['score'])\n",
    "    label_1_scores.append(out[1]['score'])\n",
    "    label_2_scores.append(out[2]['score'])\n",
    "pred_train_df = pd.DataFrame({\"split\": lab_FN_winner[:][\"0\"], \"0\":label_0_scores, \"1\": label_1_scores, \"2\": label_2_scores})\n",
    "_1 = pred_train_df[pred_train_df[\"0\"] > conf_score].copy()\n",
    "a_1 = pred_train_df[pred_train_df[\"1\"] > conf_score].copy()\n",
    "d_1 = pred_train_df[pred_train_df[\"2\"] > conf_score].copy()\n",
    "_1[\"labels\"] = 0\n",
    "a_1[\"labels\"] = 1\n",
    "d_1[\"labels\"] = 2\n",
    "predict_1 = pd.concat([_1,a_1, d_1]).sample(frac=1)\n",
    "predict_1.rename(columns={'split':'Sentence'}, inplace=True)\n",
    "predict_1.to_csv(\"label_predicted_enhanced_set.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_num = 3\n",
    "enhanced = load_tokenize_enhanced_set(\"label_predicted_enhanced_set.csv\")\n",
    "orig = load_tokenize_enhanced_set(\"../Ablation_Studies/forum/label/train_label_preproc.csv\")\n",
    "train = concatenate_datasets([enhanced, orig]).shuffle()\n",
    "test = load_tokenize_enhanced_set(\"../Ablation_Studies/forum/label/test_label_preproc.csv\")\n",
    "\n",
    "training_args = TrainingArguments(\"original_forum\", evaluation_strategy=\"epoch\")\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"classif/final_label_model\", num_labels=label_num)\n",
    "trainer = Trainer(model=model, args=training_args, train_dataset=train, eval_dataset=test, compute_metrics=compute_metrics)\n",
    "\n",
    "print(\"original:\")\n",
    "print(trainer.evaluate())\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"bert-base-german-cased\", num_labels=label_num)\n",
    "trainer = Trainer(model=model, args=training_args, train_dataset=train, eval_dataset=test, compute_metrics=compute_metrics)\n",
    "trainer.train()\n",
    "\n",
    "print(\"with automatically annotated data:\")\n",
    "print(trainer.evaluate())\n",
    "            \n",
    "trainer.save_model(\"classif/final_label_fuzzy_model_ooD\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Sublabel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# classify FN first\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"../MI_Data/Bert_Finetuning/models/FN_finetuned_gbert/model\", local_files_only=True)\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"deepset/gbert-base\")\n",
    "pipe = TextClassificationPipeline(model=model, tokenizer=tokenizer, return_all_scores=True, device=0)\n",
    "sublab_FN_winner = load_dataset(\"csv\", data_files=\"Sublabel_all_new_data.csv\", split=\"train\")\n",
    "\n",
    "label_0_scores = []\n",
    "label_1_scores = []\n",
    "    \n",
    "for out in pipe(KeyDataset(sublab_FN_winner, \"0\"), batch_size=8, truncation=\"only_first\"):\n",
    "    label_0_scores.append(out[0]['score'])\n",
    "    label_1_scores.append(out[1]['score'])\n",
    "pred_train_df = pd.DataFrame({\"Sentence\": sublab_FN_winner[:][\"0\"], \"0\":label_0_scores, \"1\": label_1_scores})\n",
    "a_1 = pred_train_df[pred_train_df[\"1\"] > 0.7].copy()\n",
    "a_1[\"labels\"] = 1\n",
    "predict_1 = a_1[\"Sentence\"]\n",
    "predict_1.to_csv(\"sublabel_FN_predicted_enhanced_set.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default-39c3c29d3fec060e\n",
      "Found cached dataset csv (/home/jovyan/.cache/huggingface/datasets/csv/default-39c3c29d3fec060e/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317)\n",
      "/opt/conda/lib/python3.9/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar funcionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "label_num = 4\n",
    "sublab_FN_winner = load_dataset(\"csv\", data_files=\"sublabel_FN_predicted_enhanced_set.csv\", split=\"train\")\n",
    "conf_score = 0.5\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"classif/final_sublabel_model\", num_labels=label_num)\n",
    "pipe = TextClassificationPipeline(model=model, tokenizer=tokenizer, return_all_scores=True, device=0)\n",
    "\n",
    "label_0_scores = []\n",
    "label_1_scores = []\n",
    "label_2_scores = []\n",
    "label_3_scores = []\n",
    "for out in pipe(KeyDataset(sublab_FN_winner, \"Sentence\"), batch_size=8, truncation=\"only_first\"):\n",
    "    label_0_scores.append(out[0]['score'])\n",
    "    label_1_scores.append(out[1]['score'])\n",
    "    label_2_scores.append(out[2]['score'])\n",
    "    label_3_scores.append(out[3]['score'])\n",
    "pred_train_df = pd.DataFrame({\"Sentence\": sublab_FN_winner[:]['Sentence'], \"0\":label_0_scores, \"1\": label_1_scores, \"2\": label_2_scores, \"3\": label_3_scores})\n",
    "#_1 = pred_train_df[pred_train_df[\"0\"] > conf_score].copy()\n",
    "#a_1 = pred_train_df[pred_train_df[\"1\"] > conf_score].copy()\n",
    "#d_1 = pred_train_df[pred_train_df[\"2\"] > conf_score].copy()\n",
    "n_1 = pred_train_df[pred_train_df[\"3\"] > conf_score].copy()\n",
    "#_1[\"labels\"] = 0\n",
    "#a_1[\"labels\"] = 1\n",
    "#d_1[\"labels\"] = 2\n",
    "n_1[\"labels\"] = 3\n",
    "#predict_1 = pd.concat([_1,a_1, d_1, n_1]).sample(frac=1)\n",
    "#n_1.rename(columns={'split':'Sentence'}, inplace=True)\n",
    "n_1 = n_1.sample(frac=0.2)\n",
    "n_1.to_csv(\"sublabel_predicted_enhanced_set.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default-892611426219477b\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset csv/default to /home/jovyan/.cache/huggingface/datasets/csv/default-892611426219477b/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c0a9760361de4496aa0f8a44a01448fb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data files:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3f22ce7ee7a347b8a91e9431acd91129",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Extracting data files:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset csv downloaded and prepared to /home/jovyan/.cache/huggingface/datasets/csv/default-892611426219477b/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "04542bacaa394a1fa7483cbe5ef3aaac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default-3edc0ad1ac61e499\n",
      "Found cached dataset csv (/home/jovyan/.cache/huggingface/datasets/csv/default-3edc0ad1ac61e499/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317)\n",
      "Loading cached processed dataset at /home/jovyan/.cache/huggingface/datasets/csv/default-3edc0ad1ac61e499/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-29cb2f8bd278accb.arrow\n",
      "Using custom data configuration default-4bcdf5d906130417\n",
      "Found cached dataset csv (/home/jovyan/.cache/huggingface/datasets/csv/default-4bcdf5d906130417/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317)\n",
      "Loading cached processed dataset at /home/jovyan/.cache/huggingface/datasets/csv/default-4bcdf5d906130417/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-037c62e99eb7ee6d.arrow\n",
      "Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: nonfunction_Sentence, non_noun_adj, function_Sentence, non_noun, non_noun_verb_adj, non_verb, Sentence, non_verb_aux. If nonfunction_Sentence, non_noun_adj, function_Sentence, non_noun, non_noun_verb_adj, non_verb, Sentence, non_verb_aux are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 596\n",
      "  Batch size = 16\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='38' max='38' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [38/38 00:05]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.6544037461280823, 'eval_accuracy': 0.8305369127516778, 'eval_f1': 0.7612787008432829, 'eval_precision': 0.7555873997662892, 'eval_recall': 0.7688653289513357, 'eval_MCC': 0.6507365408430297, 'eval_runtime': 7.6889, 'eval_samples_per_second': 77.514, 'eval_steps_per_second': 4.942}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file config.json from cache at /home/jovyan/.cache/huggingface/hub/models--bert-base-german-cased/snapshots/702774c02b32a4f360d5fea60ab034d64bf0141c/config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"bert-base-german-cased\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\",\n",
      "    \"3\": \"LABEL_3\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2,\n",
      "    \"LABEL_3\": 3\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.25.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30000\n",
      "}\n",
      "\n",
      "loading weights file pytorch_model.bin from cache at /home/jovyan/.cache/huggingface/hub/models--bert-base-german-cased/snapshots/702774c02b32a4f360d5fea60ab034d64bf0141c/pytorch_model.bin\n",
      "Some weights of the model checkpoint at bert-base-german-cased were not used when initializing BertForSequenceClassification: ['cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-german-cased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "The following columns in the training set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: nonfunction_Sentence, 2, 3, non_noun_adj, 1, function_Sentence, non_noun, 0, non_noun_verb_adj, non_verb, Sentence, non_verb_aux. If nonfunction_Sentence, 2, 3, non_noun_adj, 1, function_Sentence, non_noun, 0, non_noun_verb_adj, non_verb, Sentence, non_verb_aux are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "/opt/conda/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 2428\n",
      "  Num Epochs = 3\n",
      "  Instantaneous batch size per device = 8\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 16\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 456\n",
      "  Number of trainable parameters = 109084420\n",
      "/opt/conda/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='456' max='456' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [456/456 02:40, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>Mcc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.602056</td>\n",
       "      <td>0.766779</td>\n",
       "      <td>0.619100</td>\n",
       "      <td>0.668901</td>\n",
       "      <td>0.685631</td>\n",
       "      <td>0.513204</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.551523</td>\n",
       "      <td>0.808725</td>\n",
       "      <td>0.722760</td>\n",
       "      <td>0.726908</td>\n",
       "      <td>0.728943</td>\n",
       "      <td>0.599459</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.689305</td>\n",
       "      <td>0.798658</td>\n",
       "      <td>0.725274</td>\n",
       "      <td>0.715430</td>\n",
       "      <td>0.736582</td>\n",
       "      <td>0.591054</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: nonfunction_Sentence, non_noun_adj, function_Sentence, non_noun, non_noun_verb_adj, non_verb, Sentence, non_verb_aux. If nonfunction_Sentence, non_noun_adj, function_Sentence, non_noun, non_noun_verb_adj, non_verb, Sentence, non_verb_aux are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 596\n",
      "  Batch size = 16\n",
      "/opt/conda/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: nonfunction_Sentence, non_noun_adj, function_Sentence, non_noun, non_noun_verb_adj, non_verb, Sentence, non_verb_aux. If nonfunction_Sentence, non_noun_adj, function_Sentence, non_noun, non_noun_verb_adj, non_verb, Sentence, non_verb_aux are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 596\n",
      "  Batch size = 16\n",
      "/opt/conda/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: nonfunction_Sentence, non_noun_adj, function_Sentence, non_noun, non_noun_verb_adj, non_verb, Sentence, non_verb_aux. If nonfunction_Sentence, non_noun_adj, function_Sentence, non_noun, non_noun_verb_adj, non_verb, Sentence, non_verb_aux are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 596\n",
      "  Batch size = 16\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: nonfunction_Sentence, non_noun_adj, function_Sentence, non_noun, non_noun_verb_adj, non_verb, Sentence, non_verb_aux. If nonfunction_Sentence, non_noun_adj, function_Sentence, non_noun, non_noun_verb_adj, non_verb, Sentence, non_verb_aux are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 596\n",
      "  Batch size = 16\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "with automatically annotated data:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='38' max='38' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [38/38 00:04]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to classif/final_sublabel_fuzzy_model_ooD\n",
      "Configuration saved in classif/final_sublabel_fuzzy_model_ooD/config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.6893049478530884, 'eval_accuracy': 0.7986577181208053, 'eval_f1': 0.7252735074464731, 'eval_precision': 0.7154295921537301, 'eval_recall': 0.7365822261083974, 'eval_MCC': 0.591054303471214, 'eval_runtime': 4.5168, 'eval_samples_per_second': 131.952, 'eval_steps_per_second': 8.413, 'epoch': 3.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in classif/final_sublabel_fuzzy_model_ooD/pytorch_model.bin\n"
     ]
    }
   ],
   "source": [
    "label_num = 4\n",
    "enhanced = load_tokenize_enhanced_set(\"sublabel_predicted_enhanced_set.csv\")\n",
    "orig = load_tokenize_enhanced_set(\"../Ablation_Studies/forum/sublabel/train_sublabel_preproc.csv\")\n",
    "train = concatenate_datasets([enhanced, orig]).shuffle()\n",
    "test = load_tokenize_enhanced_set(\"../Ablation_Studies/forum/sublabel/test_sublabel_preproc.csv\")\n",
    "\n",
    "training_args = TrainingArguments(\"original_forum\", evaluation_strategy=\"epoch\")\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"classif/final_sublabel_model\", num_labels=label_num)\n",
    "trainer = Trainer(model=model, args=training_args, train_dataset=train, eval_dataset=test, compute_metrics=compute_metrics)\n",
    "\n",
    "print(\"original:\")\n",
    "print(trainer.evaluate())\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"bert-base-german-cased\", num_labels=label_num)\n",
    "trainer = Trainer(model=model, args=training_args, train_dataset=train, eval_dataset=test, compute_metrics=compute_metrics)\n",
    "trainer.train()\n",
    "\n",
    "print(\"with automatically annotated data:\")\n",
    "print(trainer.evaluate())\n",
    "            \n",
    "trainer.save_model(\"classif/final_sublabel_fuzzy_model_ooD\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using \"winning\" classifiers to predict on (out-of-domain) test sets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Valence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = {}\n",
    "metrics[1] = {}\n",
    "\n",
    "forum_test = load_predict_testset(\"../Ablation_Studies/forum/valence/test_valence_preproc.csv\", model_path=\"classif/final_valence_model\")\n",
    "metrics[1][\"GloHBCD Orig - Valence\"] = compute_test_metrics(forum_test, None)\n",
    "\n",
    "forum_test = load_predict_testset(\"../Ablation_Studies/forum/valence/test_valence_preproc.csv\", model_path=\"classif/final_valence_fuzzy_model_ooD\")\n",
    "metrics[1][\"GloHBCD Winner - Valence\"] = compute_test_metrics(forum_test, None)\n",
    "\n",
    "forum_test = load_predict_testset(\"Nonsmoking_addition/ns_val_test_set.csv\", model_path=\"classif/final_valence_model\")\n",
    "metrics[1][\"Nonsmoking Orig - Valence\"] = compute_test_metrics(forum_test, None)\n",
    "\n",
    "forum_test = load_predict_testset(\"Nonsmoking_addition/ns_val_test_set.csv\", model_path=\"classif/final_valence_fuzzy_model_ooD\")\n",
    "metrics[1][\"Nonsmoking Winner - Valence\"] = compute_test_metrics(forum_test, None)\n",
    "\n",
    "forum_test = load_predict_testset(\"../Ablation_Studies/forum/valence/optifast_valence_preproc.csv\", model_path=\"classif/final_valence_model\")\n",
    "metrics[1][\"Optifast Orig - Valence\"] = compute_test_metrics(forum_test, None)\n",
    "\n",
    "forum_test = load_predict_testset(\"../Ablation_Studies/forum/valence/optifast_valence_preproc.csv\", model_path=\"classif/final_valence_fuzzy_model_ooD\")\n",
    "metrics[1][\"Optifast Winner - Valence\"] = compute_test_metrics(forum_test, None)\n",
    "\n",
    "forum_test = load_predict_testset(\"../Ablation_Studies/forum/valence/virtualcoach_valence_preproc.csv\", model_path=\"classif/final_valence_model\")\n",
    "metrics[1][\"Virtualcoach Orig - Valence\"] = compute_test_metrics(forum_test, None)\n",
    "\n",
    "forum_test = load_predict_testset(\"../Ablation_Studies/forum/valence/virtualcoach_valence_preproc.csv\", model_path=\"classif/final_valence_fuzzy_model_ooD\")\n",
    "metrics[1][\"Virtualcoach Winner - Valence\"] = compute_test_metrics(forum_test, None)\n",
    "\n",
    "forum_test = load_predict_testset(\"../Ablation_Studies/forum/valence/synthetic_valence_preproc.csv\", model_path=\"classif/final_valence_model\")\n",
    "metrics[1][\"Synthetic Orig - Valence\"] = compute_test_metrics(forum_test, None)\n",
    "\n",
    "forum_test = load_predict_testset(\"../Ablation_Studies/forum/valence/synthetic_valence_preproc.csv\", model_path=\"classif/final_valence_fuzzy_model_ooD\")\n",
    "metrics[1][\"Synthetic Winner - Valence\"] = compute_test_metrics(forum_test, None)\n",
    "\n",
    "forum_test = load_predict_testset(\"../Ablation_Studies/forum/valence/WoZ_valence_preproc.csv\", model_path=\"classif/final_valence_model\")\n",
    "metrics[1][\"WoZ Orig - Valence\"] = compute_test_metrics(forum_test, None)\n",
    "\n",
    "forum_test = load_predict_testset(\"../Ablation_Studies/forum/valence/WoZ_valence_preproc.csv\", model_path=\"classif/final_valence_fuzzy_model_ooD\")\n",
    "metrics[1][\"WoZ Winner - Valence\"] = compute_test_metrics(forum_test, None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>accuracy</th>\n",
       "      <th>f1</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>support</th>\n",
       "      <th>MCC</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">1</th>\n",
       "      <th>GloHBCD Orig - Valence</th>\n",
       "      <td>0.782468</td>\n",
       "      <td>[0.6051080550098232, 0.8498879761015683]</td>\n",
       "      <td>[0.6724890829694323, 0.818705035971223]</td>\n",
       "      <td>[0.55, 0.8835403726708074]</td>\n",
       "      <td>[280, 644]</td>\n",
       "      <td>0.461468</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GloHBCD Winner - Valence</th>\n",
       "      <td>0.769481</td>\n",
       "      <td>[0.5831702544031311, 0.8406881077038145]</td>\n",
       "      <td>[0.645021645021645, 0.810966810966811]</td>\n",
       "      <td>[0.5321428571428571, 0.8726708074534162]</td>\n",
       "      <td>[280, 644]</td>\n",
       "      <td>0.42964</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Nonsmoking Orig - Valence</th>\n",
       "      <td>0.718593</td>\n",
       "      <td>[0.44, 0.8120805369127516]</td>\n",
       "      <td>[0.46808510638297873, 0.7960526315789473]</td>\n",
       "      <td>[0.41509433962264153, 0.8287671232876712]</td>\n",
       "      <td>[53, 146]</td>\n",
       "      <td>0.253797</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Nonsmoking Winner - Valence</th>\n",
       "      <td>0.758794</td>\n",
       "      <td>[0.5471698113207547, 0.8356164383561644]</td>\n",
       "      <td>[0.5471698113207547, 0.8356164383561644]</td>\n",
       "      <td>[0.5471698113207547, 0.8356164383561644]</td>\n",
       "      <td>[53, 146]</td>\n",
       "      <td>0.382786</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Optifast Orig - Valence</th>\n",
       "      <td>0.844156</td>\n",
       "      <td>[0.625, 0.9016393442622951]</td>\n",
       "      <td>[0.6666666666666666, 0.8870967741935484]</td>\n",
       "      <td>[0.5882352941176471, 0.9166666666666666]</td>\n",
       "      <td>[17, 60]</td>\n",
       "      <td>0.528769</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                               accuracy  \\\n",
       "1 GloHBCD Orig - Valence       0.782468   \n",
       "  GloHBCD Winner - Valence     0.769481   \n",
       "  Nonsmoking Orig - Valence    0.718593   \n",
       "  Nonsmoking Winner - Valence  0.758794   \n",
       "  Optifast Orig - Valence      0.844156   \n",
       "\n",
       "                                                                     f1  \\\n",
       "1 GloHBCD Orig - Valence       [0.6051080550098232, 0.8498879761015683]   \n",
       "  GloHBCD Winner - Valence     [0.5831702544031311, 0.8406881077038145]   \n",
       "  Nonsmoking Orig - Valence                  [0.44, 0.8120805369127516]   \n",
       "  Nonsmoking Winner - Valence  [0.5471698113207547, 0.8356164383561644]   \n",
       "  Optifast Orig - Valence                   [0.625, 0.9016393442622951]   \n",
       "\n",
       "                                                               precision  \\\n",
       "1 GloHBCD Orig - Valence         [0.6724890829694323, 0.818705035971223]   \n",
       "  GloHBCD Winner - Valence        [0.645021645021645, 0.810966810966811]   \n",
       "  Nonsmoking Orig - Valence    [0.46808510638297873, 0.7960526315789473]   \n",
       "  Nonsmoking Winner - Valence   [0.5471698113207547, 0.8356164383561644]   \n",
       "  Optifast Orig - Valence       [0.6666666666666666, 0.8870967741935484]   \n",
       "\n",
       "                                                                  recall  \\\n",
       "1 GloHBCD Orig - Valence                      [0.55, 0.8835403726708074]   \n",
       "  GloHBCD Winner - Valence      [0.5321428571428571, 0.8726708074534162]   \n",
       "  Nonsmoking Orig - Valence    [0.41509433962264153, 0.8287671232876712]   \n",
       "  Nonsmoking Winner - Valence   [0.5471698113207547, 0.8356164383561644]   \n",
       "  Optifast Orig - Valence       [0.5882352941176471, 0.9166666666666666]   \n",
       "\n",
       "                                  support       MCC  \n",
       "1 GloHBCD Orig - Valence       [280, 644]  0.461468  \n",
       "  GloHBCD Winner - Valence     [280, 644]   0.42964  \n",
       "  Nonsmoking Orig - Valence     [53, 146]  0.253797  \n",
       "  Nonsmoking Winner - Valence   [53, 146]  0.382786  \n",
       "  Optifast Orig - Valence        [17, 60]  0.528769  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics_df = pd.DataFrame.from_dict({(i,j): metrics[i][j]\n",
    "                            for i in metrics.keys() \n",
    "                            for j in metrics[i].keys()}, \n",
    "                        orient='columns').transpose()\n",
    "            \n",
    "metrics_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_df.to_csv(\"metrics/Valence_adding_new_data_test_Sets.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default-32b27a53db4c4331\n",
      "Found cached dataset csv (/home/jovyan/.cache/huggingface/datasets/csv/default-32b27a53db4c4331/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "80a3cb2a45ab41588687cc867897c740",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /home/jovyan/.cache/huggingface/datasets/csv/default-32b27a53db4c4331/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-84a7adbd3a5b54ca.arrow\n",
      "loading configuration file classif/final_label_model/config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"classif/final_label_model\",\n",
      "  \"architectures\": [\n",
      "    \"BertForSequenceClassification\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"problem_type\": \"single_label_classification\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.25.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30000\n",
      "}\n",
      "\n",
      "loading weights file classif/final_label_model/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing BertForSequenceClassification.\n",
      "\n",
      "All the weights of BertForSequenceClassification were initialized from the model checkpoint at classif/final_label_model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use BertForSequenceClassification for predictions without further training.\n",
      "The following columns in the test set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: nonfunction_Sentence, non_noun_adj, function_Sentence, non_noun, non_noun_verb_adj, non_verb, Sentence, non_verb_aux. If nonfunction_Sentence, non_noun_adj, function_Sentence, non_noun, non_noun_verb_adj, non_verb, Sentence, non_verb_aux are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Prediction *****\n",
      "  Num examples = 929\n",
      "  Batch size = 16\n",
      "/opt/conda/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default-32b27a53db4c4331\n",
      "Found cached dataset csv (/home/jovyan/.cache/huggingface/datasets/csv/default-32b27a53db4c4331/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1276f97936bf4a688118cc4d6d406dc9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /home/jovyan/.cache/huggingface/datasets/csv/default-32b27a53db4c4331/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-84a7adbd3a5b54ca.arrow\n",
      "loading configuration file classif/final_label_fuzzy_model_ooD/config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"classif/final_label_fuzzy_model_ooD\",\n",
      "  \"architectures\": [\n",
      "    \"BertForSequenceClassification\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"problem_type\": \"single_label_classification\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.25.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30000\n",
      "}\n",
      "\n",
      "loading weights file classif/final_label_fuzzy_model_ooD/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing BertForSequenceClassification.\n",
      "\n",
      "All the weights of BertForSequenceClassification were initialized from the model checkpoint at classif/final_label_fuzzy_model_ooD.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use BertForSequenceClassification for predictions without further training.\n",
      "The following columns in the test set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: nonfunction_Sentence, non_noun_adj, function_Sentence, non_noun, non_noun_verb_adj, non_verb, Sentence, non_verb_aux. If nonfunction_Sentence, non_noun_adj, function_Sentence, non_noun, non_noun_verb_adj, non_verb, Sentence, non_verb_aux are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Prediction *****\n",
      "  Num examples = 929\n",
      "  Batch size = 16\n",
      "/opt/conda/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default-ad3e62592adebd9c\n",
      "Found cached dataset csv (/home/jovyan/.cache/huggingface/datasets/csv/default-ad3e62592adebd9c/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "20337929027e4d74afa6337019d0e3c4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /home/jovyan/.cache/huggingface/datasets/csv/default-ad3e62592adebd9c/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-bdb3ba4d04ebbb9b.arrow\n",
      "loading configuration file classif/final_label_model/config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"classif/final_label_model\",\n",
      "  \"architectures\": [\n",
      "    \"BertForSequenceClassification\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"problem_type\": \"single_label_classification\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.25.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30000\n",
      "}\n",
      "\n",
      "loading weights file classif/final_label_model/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing BertForSequenceClassification.\n",
      "\n",
      "All the weights of BertForSequenceClassification were initialized from the model checkpoint at classif/final_label_model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use BertForSequenceClassification for predictions without further training.\n",
      "The following columns in the test set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: nonfunction_Sentence, non_noun_adj, Unnamed: 0.1, function_Sentence, non_noun, non_noun_verb_adj, non_verb, Sentence, non_verb_aux. If nonfunction_Sentence, non_noun_adj, Unnamed: 0.1, function_Sentence, non_noun, non_noun_verb_adj, non_verb, Sentence, non_verb_aux are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Prediction *****\n",
      "  Num examples = 199\n",
      "  Batch size = 16\n",
      "/opt/conda/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default-ad3e62592adebd9c\n",
      "Found cached dataset csv (/home/jovyan/.cache/huggingface/datasets/csv/default-ad3e62592adebd9c/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7680eef4b1134d639ae164069d1cf890",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /home/jovyan/.cache/huggingface/datasets/csv/default-ad3e62592adebd9c/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-bdb3ba4d04ebbb9b.arrow\n",
      "loading configuration file classif/final_label_fuzzy_model_ooD/config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"classif/final_label_fuzzy_model_ooD\",\n",
      "  \"architectures\": [\n",
      "    \"BertForSequenceClassification\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"problem_type\": \"single_label_classification\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.25.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30000\n",
      "}\n",
      "\n",
      "loading weights file classif/final_label_fuzzy_model_ooD/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing BertForSequenceClassification.\n",
      "\n",
      "All the weights of BertForSequenceClassification were initialized from the model checkpoint at classif/final_label_fuzzy_model_ooD.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use BertForSequenceClassification for predictions without further training.\n",
      "The following columns in the test set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: nonfunction_Sentence, non_noun_adj, Unnamed: 0.1, function_Sentence, non_noun, non_noun_verb_adj, non_verb, Sentence, non_verb_aux. If nonfunction_Sentence, non_noun_adj, Unnamed: 0.1, function_Sentence, non_noun, non_noun_verb_adj, non_verb, Sentence, non_verb_aux are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Prediction *****\n",
      "  Num examples = 199\n",
      "  Batch size = 16\n",
      "/opt/conda/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default-359ddd787f08c427\n",
      "Found cached dataset csv (/home/jovyan/.cache/huggingface/datasets/csv/default-359ddd787f08c427/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2b7cbe5dde60456e9da15df90d8e7f5e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /home/jovyan/.cache/huggingface/datasets/csv/default-359ddd787f08c427/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-bfa0112546168774.arrow\n",
      "loading configuration file classif/final_label_model/config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"classif/final_label_model\",\n",
      "  \"architectures\": [\n",
      "    \"BertForSequenceClassification\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"problem_type\": \"single_label_classification\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.25.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30000\n",
      "}\n",
      "\n",
      "loading weights file classif/final_label_model/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing BertForSequenceClassification.\n",
      "\n",
      "All the weights of BertForSequenceClassification were initialized from the model checkpoint at classif/final_label_model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use BertForSequenceClassification for predictions without further training.\n",
      "The following columns in the test set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: nonfunction_Sentence, non_noun_adj, function_Sentence, non_noun, non_noun_verb_adj, non_verb, Sentence, non_verb_aux. If nonfunction_Sentence, non_noun_adj, function_Sentence, non_noun, non_noun_verb_adj, non_verb, Sentence, non_verb_aux are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Prediction *****\n",
      "  Num examples = 77\n",
      "  Batch size = 16\n",
      "/opt/conda/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default-359ddd787f08c427\n",
      "Found cached dataset csv (/home/jovyan/.cache/huggingface/datasets/csv/default-359ddd787f08c427/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9242f75a5eaa4f9f945f20aaa8460ea5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /home/jovyan/.cache/huggingface/datasets/csv/default-359ddd787f08c427/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-bfa0112546168774.arrow\n",
      "loading configuration file classif/final_label_fuzzy_model_ooD/config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"classif/final_label_fuzzy_model_ooD\",\n",
      "  \"architectures\": [\n",
      "    \"BertForSequenceClassification\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"problem_type\": \"single_label_classification\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.25.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30000\n",
      "}\n",
      "\n",
      "loading weights file classif/final_label_fuzzy_model_ooD/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing BertForSequenceClassification.\n",
      "\n",
      "All the weights of BertForSequenceClassification were initialized from the model checkpoint at classif/final_label_fuzzy_model_ooD.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use BertForSequenceClassification for predictions without further training.\n",
      "The following columns in the test set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: nonfunction_Sentence, non_noun_adj, function_Sentence, non_noun, non_noun_verb_adj, non_verb, Sentence, non_verb_aux. If nonfunction_Sentence, non_noun_adj, function_Sentence, non_noun, non_noun_verb_adj, non_verb, Sentence, non_verb_aux are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Prediction *****\n",
      "  Num examples = 77\n",
      "  Batch size = 16\n",
      "/opt/conda/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default-00831dc8ce6d964a\n",
      "Found cached dataset csv (/home/jovyan/.cache/huggingface/datasets/csv/default-00831dc8ce6d964a/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2c26642e5edd4c82bfa10113b5938020",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /home/jovyan/.cache/huggingface/datasets/csv/default-00831dc8ce6d964a/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-abdd711b8f62df8c.arrow\n",
      "loading configuration file classif/final_label_model/config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"classif/final_label_model\",\n",
      "  \"architectures\": [\n",
      "    \"BertForSequenceClassification\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"problem_type\": \"single_label_classification\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.25.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30000\n",
      "}\n",
      "\n",
      "loading weights file classif/final_label_model/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing BertForSequenceClassification.\n",
      "\n",
      "All the weights of BertForSequenceClassification were initialized from the model checkpoint at classif/final_label_model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use BertForSequenceClassification for predictions without further training.\n",
      "The following columns in the test set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: nonfunction_Sentence, non_noun_adj, function_Sentence, non_noun, non_noun_verb_adj, non_verb, Sentence, non_verb_aux. If nonfunction_Sentence, non_noun_adj, function_Sentence, non_noun, non_noun_verb_adj, non_verb, Sentence, non_verb_aux are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Prediction *****\n",
      "  Num examples = 493\n",
      "  Batch size = 16\n",
      "/opt/conda/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default-00831dc8ce6d964a\n",
      "Found cached dataset csv (/home/jovyan/.cache/huggingface/datasets/csv/default-00831dc8ce6d964a/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3b5ff078798042edb235f3acf6c13254",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /home/jovyan/.cache/huggingface/datasets/csv/default-00831dc8ce6d964a/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-abdd711b8f62df8c.arrow\n",
      "loading configuration file classif/final_label_fuzzy_model_ooD/config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"classif/final_label_fuzzy_model_ooD\",\n",
      "  \"architectures\": [\n",
      "    \"BertForSequenceClassification\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"problem_type\": \"single_label_classification\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.25.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30000\n",
      "}\n",
      "\n",
      "loading weights file classif/final_label_fuzzy_model_ooD/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing BertForSequenceClassification.\n",
      "\n",
      "All the weights of BertForSequenceClassification were initialized from the model checkpoint at classif/final_label_fuzzy_model_ooD.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use BertForSequenceClassification for predictions without further training.\n",
      "The following columns in the test set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: nonfunction_Sentence, non_noun_adj, function_Sentence, non_noun, non_noun_verb_adj, non_verb, Sentence, non_verb_aux. If nonfunction_Sentence, non_noun_adj, function_Sentence, non_noun, non_noun_verb_adj, non_verb, Sentence, non_verb_aux are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Prediction *****\n",
      "  Num examples = 493\n",
      "  Batch size = 16\n",
      "/opt/conda/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default-7de2ad3c7bd977df\n",
      "Found cached dataset csv (/home/jovyan/.cache/huggingface/datasets/csv/default-7de2ad3c7bd977df/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f6339ea1eba04dacaaa19c7a89b51a68",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /home/jovyan/.cache/huggingface/datasets/csv/default-7de2ad3c7bd977df/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-60f1963e80c048cf.arrow\n",
      "loading configuration file classif/final_label_model/config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"classif/final_label_model\",\n",
      "  \"architectures\": [\n",
      "    \"BertForSequenceClassification\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"problem_type\": \"single_label_classification\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.25.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30000\n",
      "}\n",
      "\n",
      "loading weights file classif/final_label_model/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing BertForSequenceClassification.\n",
      "\n",
      "All the weights of BertForSequenceClassification were initialized from the model checkpoint at classif/final_label_model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use BertForSequenceClassification for predictions without further training.\n",
      "The following columns in the test set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: nonfunction_Sentence, non_noun_adj, function_Sentence, non_noun, non_noun_verb_adj, non_verb, Sentence, non_verb_aux. If nonfunction_Sentence, non_noun_adj, function_Sentence, non_noun, non_noun_verb_adj, non_verb, Sentence, non_verb_aux are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Prediction *****\n",
      "  Num examples = 74\n",
      "  Batch size = 16\n",
      "/opt/conda/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default-7de2ad3c7bd977df\n",
      "Found cached dataset csv (/home/jovyan/.cache/huggingface/datasets/csv/default-7de2ad3c7bd977df/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bd043f72572b45899e627ddbee8b6874",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /home/jovyan/.cache/huggingface/datasets/csv/default-7de2ad3c7bd977df/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-60f1963e80c048cf.arrow\n",
      "loading configuration file classif/final_label_fuzzy_model_ooD/config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"classif/final_label_fuzzy_model_ooD\",\n",
      "  \"architectures\": [\n",
      "    \"BertForSequenceClassification\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"problem_type\": \"single_label_classification\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.25.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30000\n",
      "}\n",
      "\n",
      "loading weights file classif/final_label_fuzzy_model_ooD/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing BertForSequenceClassification.\n",
      "\n",
      "All the weights of BertForSequenceClassification were initialized from the model checkpoint at classif/final_label_fuzzy_model_ooD.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use BertForSequenceClassification for predictions without further training.\n",
      "The following columns in the test set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: nonfunction_Sentence, non_noun_adj, function_Sentence, non_noun, non_noun_verb_adj, non_verb, Sentence, non_verb_aux. If nonfunction_Sentence, non_noun_adj, function_Sentence, non_noun, non_noun_verb_adj, non_verb, Sentence, non_verb_aux are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Prediction *****\n",
      "  Num examples = 74\n",
      "  Batch size = 16\n",
      "/opt/conda/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default-4dbcc69c91c16d9d\n",
      "Found cached dataset csv (/home/jovyan/.cache/huggingface/datasets/csv/default-4dbcc69c91c16d9d/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9a6482e06cef4293b0188f7db5358b1f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /home/jovyan/.cache/huggingface/datasets/csv/default-4dbcc69c91c16d9d/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-f3e5e2f46ca24636.arrow\n",
      "loading configuration file classif/final_label_model/config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"classif/final_label_model\",\n",
      "  \"architectures\": [\n",
      "    \"BertForSequenceClassification\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"problem_type\": \"single_label_classification\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.25.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30000\n",
      "}\n",
      "\n",
      "loading weights file classif/final_label_model/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing BertForSequenceClassification.\n",
      "\n",
      "All the weights of BertForSequenceClassification were initialized from the model checkpoint at classif/final_label_model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use BertForSequenceClassification for predictions without further training.\n",
      "The following columns in the test set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: nonfunction_Sentence, non_noun_adj, function_Sentence, non_noun, non_noun_verb_adj, non_verb, Sentence, non_verb_aux. If nonfunction_Sentence, non_noun_adj, function_Sentence, non_noun, non_noun_verb_adj, non_verb, Sentence, non_verb_aux are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Prediction *****\n",
      "  Num examples = 71\n",
      "  Batch size = 16\n",
      "/opt/conda/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default-4dbcc69c91c16d9d\n",
      "Found cached dataset csv (/home/jovyan/.cache/huggingface/datasets/csv/default-4dbcc69c91c16d9d/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "540e3d4215f348a5881d9fe1eab70adc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /home/jovyan/.cache/huggingface/datasets/csv/default-4dbcc69c91c16d9d/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-f3e5e2f46ca24636.arrow\n",
      "loading configuration file classif/final_label_fuzzy_model_ooD/config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"classif/final_label_fuzzy_model_ooD\",\n",
      "  \"architectures\": [\n",
      "    \"BertForSequenceClassification\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"problem_type\": \"single_label_classification\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.25.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30000\n",
      "}\n",
      "\n",
      "loading weights file classif/final_label_fuzzy_model_ooD/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing BertForSequenceClassification.\n",
      "\n",
      "All the weights of BertForSequenceClassification were initialized from the model checkpoint at classif/final_label_fuzzy_model_ooD.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use BertForSequenceClassification for predictions without further training.\n",
      "The following columns in the test set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: nonfunction_Sentence, non_noun_adj, function_Sentence, non_noun, non_noun_verb_adj, non_verb, Sentence, non_verb_aux. If nonfunction_Sentence, non_noun_adj, function_Sentence, non_noun, non_noun_verb_adj, non_verb, Sentence, non_verb_aux are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Prediction *****\n",
      "  Num examples = 71\n",
      "  Batch size = 16\n",
      "/opt/conda/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "metrics = {}\n",
    "metrics[1] = {}\n",
    "\n",
    "forum_test = load_predict_testset(\"../Ablation_Studies/forum/label/test_label_preproc.csv\", model_path=\"classif/final_label_model\")\n",
    "metrics[1][\"GloHBCD Orig - label\"] = compute_test_metrics(forum_test, None)\n",
    "\n",
    "forum_test = load_predict_testset(\"../Ablation_Studies/forum/label/test_label_preproc.csv\", model_path=\"classif/final_label_fuzzy_model_ooD\")\n",
    "metrics[1][\"GloHBCD Winner - label\"] = compute_test_metrics(forum_test, None)\n",
    "\n",
    "forum_test = load_predict_testset(\"Nonsmoking_addition/ns_label_test_set.csv\", model_path=\"classif/final_label_model\")\n",
    "metrics[1][\"Nonsmoking Orig - label\"] = compute_test_metrics(forum_test, None)\n",
    "\n",
    "forum_test = load_predict_testset(\"Nonsmoking_addition/ns_label_test_set.csv\", model_path=\"classif/final_label_fuzzy_model_ooD\")\n",
    "metrics[1][\"Nonsmoking Winner - label\"] = compute_test_metrics(forum_test, None)\n",
    "\n",
    "forum_test = load_predict_testset(\"../Ablation_Studies/forum/label/optifast_label_preproc.csv\", model_path=\"classif/final_label_model\")\n",
    "metrics[1][\"Optifast Orig - label\"] = compute_test_metrics(forum_test, None)\n",
    "\n",
    "forum_test = load_predict_testset(\"../Ablation_Studies/forum/label/optifast_label_preproc.csv\", model_path=\"classif/final_label_fuzzy_model_ooD\")\n",
    "metrics[1][\"Optifast Winner - label\"] = compute_test_metrics(forum_test, None)\n",
    "\n",
    "forum_test = load_predict_testset(\"../Ablation_Studies/forum/label/virtualcoach_label_preproc.csv\", model_path=\"classif/final_label_model\")\n",
    "metrics[1][\"Virtualcoach Orig - label\"] = compute_test_metrics(forum_test, None)\n",
    "\n",
    "forum_test = load_predict_testset(\"../Ablation_Studies/forum/label/virtualcoach_label_preproc.csv\", model_path=\"classif/final_label_fuzzy_model_ooD\")\n",
    "metrics[1][\"Virtualcoach Winner - label\"] = compute_test_metrics(forum_test, None)\n",
    "\n",
    "forum_test = load_predict_testset(\"../Ablation_Studies/forum/label/synthetic_label_preproc.csv\", model_path=\"classif/final_label_model\")\n",
    "metrics[1][\"Synthetic Orig - label\"] = compute_test_metrics(forum_test, None)\n",
    "\n",
    "forum_test = load_predict_testset(\"../Ablation_Studies/forum/label/synthetic_label_preproc.csv\", model_path=\"classif/final_label_fuzzy_model_ooD\")\n",
    "metrics[1][\"Synthetic Winner - label\"] = compute_test_metrics(forum_test, None)\n",
    "\n",
    "forum_test = load_predict_testset(\"../Ablation_Studies/forum/label/WoZ_label_preproc.csv\", model_path=\"classif/final_label_model\")\n",
    "metrics[1][\"WoZ Orig - label\"] = compute_test_metrics(forum_test, None)\n",
    "\n",
    "forum_test = load_predict_testset(\"../Ablation_Studies/forum/label/WoZ_label_preproc.csv\", model_path=\"classif/final_label_fuzzy_model_ooD\")\n",
    "metrics[1][\"WoZ Winner - label\"] = compute_test_metrics(forum_test, None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_lab = pd.DataFrame.from_dict({(i,j): metrics[i][j]\n",
    "                            for i in metrics.keys() \n",
    "                            for j in metrics[i].keys()}, \n",
    "                        orient='columns').transpose()\n",
    "\n",
    "#metrics_df = pd.concat([metrics_df, metrics_lab])\n",
    "#metrics_df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_lab.to_csv(\"metrics/Label_adding_new_data_test_Sets.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default-4bcdf5d906130417\n",
      "Found cached dataset csv (/home/jovyan/.cache/huggingface/datasets/csv/default-4bcdf5d906130417/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0284910a72af4ce692643b0353d0f9ee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /home/jovyan/.cache/huggingface/datasets/csv/default-4bcdf5d906130417/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-037c62e99eb7ee6d.arrow\n",
      "loading configuration file classif/final_sublabel_model/config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"classif/final_sublabel_model\",\n",
      "  \"architectures\": [\n",
      "    \"BertForSequenceClassification\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\",\n",
      "    \"3\": \"LABEL_3\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2,\n",
      "    \"LABEL_3\": 3\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"problem_type\": \"single_label_classification\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.25.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30000\n",
      "}\n",
      "\n",
      "loading weights file classif/final_sublabel_model/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing BertForSequenceClassification.\n",
      "\n",
      "All the weights of BertForSequenceClassification were initialized from the model checkpoint at classif/final_sublabel_model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use BertForSequenceClassification for predictions without further training.\n",
      "The following columns in the test set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: nonfunction_Sentence, non_noun_adj, function_Sentence, non_noun, non_noun_verb_adj, non_verb, Sentence, non_verb_aux. If nonfunction_Sentence, non_noun_adj, function_Sentence, non_noun, non_noun_verb_adj, non_verb, Sentence, non_verb_aux are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Prediction *****\n",
      "  Num examples = 596\n",
      "  Batch size = 16\n",
      "/opt/conda/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default-4bcdf5d906130417\n",
      "Found cached dataset csv (/home/jovyan/.cache/huggingface/datasets/csv/default-4bcdf5d906130417/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6713c88f477b4c6692614685a9a48391",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /home/jovyan/.cache/huggingface/datasets/csv/default-4bcdf5d906130417/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-037c62e99eb7ee6d.arrow\n",
      "loading configuration file classif/final_sublabel_fuzzy_model_ooD/config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"classif/final_sublabel_fuzzy_model_ooD\",\n",
      "  \"architectures\": [\n",
      "    \"BertForSequenceClassification\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\",\n",
      "    \"3\": \"LABEL_3\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2,\n",
      "    \"LABEL_3\": 3\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"problem_type\": \"single_label_classification\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.25.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30000\n",
      "}\n",
      "\n",
      "loading weights file classif/final_sublabel_fuzzy_model_ooD/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing BertForSequenceClassification.\n",
      "\n",
      "All the weights of BertForSequenceClassification were initialized from the model checkpoint at classif/final_sublabel_fuzzy_model_ooD.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use BertForSequenceClassification for predictions without further training.\n",
      "The following columns in the test set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: nonfunction_Sentence, non_noun_adj, function_Sentence, non_noun, non_noun_verb_adj, non_verb, Sentence, non_verb_aux. If nonfunction_Sentence, non_noun_adj, function_Sentence, non_noun, non_noun_verb_adj, non_verb, Sentence, non_verb_aux are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Prediction *****\n",
      "  Num examples = 596\n",
      "  Batch size = 16\n",
      "/opt/conda/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default-cd4e11e321514c47\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset csv/default to /home/jovyan/.cache/huggingface/datasets/csv/default-cd4e11e321514c47/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c6fcb7941d4d41adbe5de630458e97c3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data files:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0855844b1e4b4ac3ade52c11eaaac328",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Extracting data files:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset csv downloaded and prepared to /home/jovyan/.cache/huggingface/datasets/csv/default-cd4e11e321514c47/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3c320eaed1fa48fe9e1ea7b6fdde2b67",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ce3abdee3d2e4a6e93c19f6e423fa039",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file classif/final_sublabel_model/config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"classif/final_sublabel_model\",\n",
      "  \"architectures\": [\n",
      "    \"BertForSequenceClassification\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\",\n",
      "    \"3\": \"LABEL_3\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2,\n",
      "    \"LABEL_3\": 3\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"problem_type\": \"single_label_classification\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.25.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30000\n",
      "}\n",
      "\n",
      "loading weights file classif/final_sublabel_model/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing BertForSequenceClassification.\n",
      "\n",
      "All the weights of BertForSequenceClassification were initialized from the model checkpoint at classif/final_sublabel_model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use BertForSequenceClassification for predictions without further training.\n",
      "The following columns in the test set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: nonfunction_Sentence, non_noun_adj, Unnamed: 0.1, function_Sentence, non_noun, non_noun_verb_adj, non_verb, Sentence, non_verb_aux. If nonfunction_Sentence, non_noun_adj, Unnamed: 0.1, function_Sentence, non_noun, non_noun_verb_adj, non_verb, Sentence, non_verb_aux are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Prediction *****\n",
      "  Num examples = 131\n",
      "  Batch size = 16\n",
      "/opt/conda/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default-cd4e11e321514c47\n",
      "Found cached dataset csv (/home/jovyan/.cache/huggingface/datasets/csv/default-cd4e11e321514c47/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1efde6beacb04b28974e5f4660cd9924",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /home/jovyan/.cache/huggingface/datasets/csv/default-cd4e11e321514c47/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-05482541103b2312.arrow\n",
      "loading configuration file classif/final_sublabel_fuzzy_model_ooD/config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"classif/final_sublabel_fuzzy_model_ooD\",\n",
      "  \"architectures\": [\n",
      "    \"BertForSequenceClassification\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\",\n",
      "    \"3\": \"LABEL_3\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2,\n",
      "    \"LABEL_3\": 3\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"problem_type\": \"single_label_classification\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.25.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30000\n",
      "}\n",
      "\n",
      "loading weights file classif/final_sublabel_fuzzy_model_ooD/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing BertForSequenceClassification.\n",
      "\n",
      "All the weights of BertForSequenceClassification were initialized from the model checkpoint at classif/final_sublabel_fuzzy_model_ooD.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use BertForSequenceClassification for predictions without further training.\n",
      "The following columns in the test set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: nonfunction_Sentence, non_noun_adj, Unnamed: 0.1, function_Sentence, non_noun, non_noun_verb_adj, non_verb, Sentence, non_verb_aux. If nonfunction_Sentence, non_noun_adj, Unnamed: 0.1, function_Sentence, non_noun, non_noun_verb_adj, non_verb, Sentence, non_verb_aux are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Prediction *****\n",
      "  Num examples = 131\n",
      "  Batch size = 16\n",
      "/opt/conda/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default-96915e9bf8fdc22c\n",
      "Found cached dataset csv (/home/jovyan/.cache/huggingface/datasets/csv/default-96915e9bf8fdc22c/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c9b39b2e4fea4373a824cd766dcf61a1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b840c6cfb4dd4d50ad04abcea2974a11",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file classif/final_sublabel_model/config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"classif/final_sublabel_model\",\n",
      "  \"architectures\": [\n",
      "    \"BertForSequenceClassification\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\",\n",
      "    \"3\": \"LABEL_3\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2,\n",
      "    \"LABEL_3\": 3\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"problem_type\": \"single_label_classification\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.25.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30000\n",
      "}\n",
      "\n",
      "loading weights file classif/final_sublabel_model/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing BertForSequenceClassification.\n",
      "\n",
      "All the weights of BertForSequenceClassification were initialized from the model checkpoint at classif/final_sublabel_model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use BertForSequenceClassification for predictions without further training.\n",
      "The following columns in the test set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: nonfunction_Sentence, non_noun_adj, function_Sentence, non_noun, non_noun_verb_adj, non_verb, Sentence, non_verb_aux. If nonfunction_Sentence, non_noun_adj, function_Sentence, non_noun, non_noun_verb_adj, non_verb, Sentence, non_verb_aux are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Prediction *****\n",
      "  Num examples = 55\n",
      "  Batch size = 16\n",
      "/opt/conda/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "Using custom data configuration default-96915e9bf8fdc22c\n",
      "Found cached dataset csv (/home/jovyan/.cache/huggingface/datasets/csv/default-96915e9bf8fdc22c/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fd64d74f87a74f4cbd4b82650d6d419a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /home/jovyan/.cache/huggingface/datasets/csv/default-96915e9bf8fdc22c/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-bde516ce57430205.arrow\n",
      "loading configuration file classif/final_sublabel_fuzzy_model_ooD/config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"classif/final_sublabel_fuzzy_model_ooD\",\n",
      "  \"architectures\": [\n",
      "    \"BertForSequenceClassification\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\",\n",
      "    \"3\": \"LABEL_3\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2,\n",
      "    \"LABEL_3\": 3\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"problem_type\": \"single_label_classification\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.25.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30000\n",
      "}\n",
      "\n",
      "loading weights file classif/final_sublabel_fuzzy_model_ooD/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing BertForSequenceClassification.\n",
      "\n",
      "All the weights of BertForSequenceClassification were initialized from the model checkpoint at classif/final_sublabel_fuzzy_model_ooD.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use BertForSequenceClassification for predictions without further training.\n",
      "The following columns in the test set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: nonfunction_Sentence, non_noun_adj, function_Sentence, non_noun, non_noun_verb_adj, non_verb, Sentence, non_verb_aux. If nonfunction_Sentence, non_noun_adj, function_Sentence, non_noun, non_noun_verb_adj, non_verb, Sentence, non_verb_aux are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Prediction *****\n",
      "  Num examples = 55\n",
      "  Batch size = 16\n",
      "/opt/conda/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "Using custom data configuration default-5f2c863d47dd7bd8\n",
      "Found cached dataset csv (/home/jovyan/.cache/huggingface/datasets/csv/default-5f2c863d47dd7bd8/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "feb213380d75472bbdb31e8de54d5f8e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ab9932e95eec4802be744726f018fc3e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file classif/final_sublabel_model/config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"classif/final_sublabel_model\",\n",
      "  \"architectures\": [\n",
      "    \"BertForSequenceClassification\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\",\n",
      "    \"3\": \"LABEL_3\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2,\n",
      "    \"LABEL_3\": 3\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"problem_type\": \"single_label_classification\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.25.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30000\n",
      "}\n",
      "\n",
      "loading weights file classif/final_sublabel_model/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing BertForSequenceClassification.\n",
      "\n",
      "All the weights of BertForSequenceClassification were initialized from the model checkpoint at classif/final_sublabel_model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use BertForSequenceClassification for predictions without further training.\n",
      "The following columns in the test set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: nonfunction_Sentence, non_noun_adj, function_Sentence, non_noun, non_noun_verb_adj, non_verb, Sentence, non_verb_aux. If nonfunction_Sentence, non_noun_adj, function_Sentence, non_noun, non_noun_verb_adj, non_verb, Sentence, non_verb_aux are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Prediction *****\n",
      "  Num examples = 167\n",
      "  Batch size = 16\n",
      "/opt/conda/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default-5f2c863d47dd7bd8\n",
      "Found cached dataset csv (/home/jovyan/.cache/huggingface/datasets/csv/default-5f2c863d47dd7bd8/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b2c2b62fae874874bd46f2b8aa43e2b2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /home/jovyan/.cache/huggingface/datasets/csv/default-5f2c863d47dd7bd8/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-e4424748f3d07782.arrow\n",
      "loading configuration file classif/final_sublabel_fuzzy_model_ooD/config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"classif/final_sublabel_fuzzy_model_ooD\",\n",
      "  \"architectures\": [\n",
      "    \"BertForSequenceClassification\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\",\n",
      "    \"3\": \"LABEL_3\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2,\n",
      "    \"LABEL_3\": 3\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"problem_type\": \"single_label_classification\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.25.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30000\n",
      "}\n",
      "\n",
      "loading weights file classif/final_sublabel_fuzzy_model_ooD/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing BertForSequenceClassification.\n",
      "\n",
      "All the weights of BertForSequenceClassification were initialized from the model checkpoint at classif/final_sublabel_fuzzy_model_ooD.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use BertForSequenceClassification for predictions without further training.\n",
      "The following columns in the test set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: nonfunction_Sentence, non_noun_adj, function_Sentence, non_noun, non_noun_verb_adj, non_verb, Sentence, non_verb_aux. If nonfunction_Sentence, non_noun_adj, function_Sentence, non_noun, non_noun_verb_adj, non_verb, Sentence, non_verb_aux are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Prediction *****\n",
      "  Num examples = 167\n",
      "  Batch size = 16\n",
      "/opt/conda/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default-716405ebd119018a\n",
      "Found cached dataset csv (/home/jovyan/.cache/huggingface/datasets/csv/default-716405ebd119018a/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "53e48f85f4ef400b83a787079456c292",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "af484ee047044ee79331f29dfcfea565",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file classif/final_sublabel_model/config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"classif/final_sublabel_model\",\n",
      "  \"architectures\": [\n",
      "    \"BertForSequenceClassification\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\",\n",
      "    \"3\": \"LABEL_3\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2,\n",
      "    \"LABEL_3\": 3\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"problem_type\": \"single_label_classification\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.25.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30000\n",
      "}\n",
      "\n",
      "loading weights file classif/final_sublabel_model/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing BertForSequenceClassification.\n",
      "\n",
      "All the weights of BertForSequenceClassification were initialized from the model checkpoint at classif/final_sublabel_model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use BertForSequenceClassification for predictions without further training.\n",
      "The following columns in the test set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: nonfunction_Sentence, non_noun_adj, function_Sentence, non_noun, non_noun_verb_adj, non_verb, Sentence, non_verb_aux. If nonfunction_Sentence, non_noun_adj, function_Sentence, non_noun, non_noun_verb_adj, non_verb, Sentence, non_verb_aux are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Prediction *****\n",
      "  Num examples = 54\n",
      "  Batch size = 16\n",
      "/opt/conda/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default-716405ebd119018a\n",
      "Found cached dataset csv (/home/jovyan/.cache/huggingface/datasets/csv/default-716405ebd119018a/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2a18f5b60f5f42f5bdf863d56a187637",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /home/jovyan/.cache/huggingface/datasets/csv/default-716405ebd119018a/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-ec159c1d9abf741e.arrow\n",
      "loading configuration file classif/final_sublabel_fuzzy_model_ooD/config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"classif/final_sublabel_fuzzy_model_ooD\",\n",
      "  \"architectures\": [\n",
      "    \"BertForSequenceClassification\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\",\n",
      "    \"3\": \"LABEL_3\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2,\n",
      "    \"LABEL_3\": 3\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"problem_type\": \"single_label_classification\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.25.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30000\n",
      "}\n",
      "\n",
      "loading weights file classif/final_sublabel_fuzzy_model_ooD/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing BertForSequenceClassification.\n",
      "\n",
      "All the weights of BertForSequenceClassification were initialized from the model checkpoint at classif/final_sublabel_fuzzy_model_ooD.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use BertForSequenceClassification for predictions without further training.\n",
      "The following columns in the test set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: nonfunction_Sentence, non_noun_adj, function_Sentence, non_noun, non_noun_verb_adj, non_verb, Sentence, non_verb_aux. If nonfunction_Sentence, non_noun_adj, function_Sentence, non_noun, non_noun_verb_adj, non_verb, Sentence, non_verb_aux are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Prediction *****\n",
      "  Num examples = 54\n",
      "  Batch size = 16\n",
      "/opt/conda/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default-505b13c249afb61c\n",
      "Found cached dataset csv (/home/jovyan/.cache/huggingface/datasets/csv/default-505b13c249afb61c/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7a7d8c9ddb77470caeebb8ba8ae45504",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3de18f589fc742088a4c2fc37e3bfff6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file classif/final_sublabel_model/config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"classif/final_sublabel_model\",\n",
      "  \"architectures\": [\n",
      "    \"BertForSequenceClassification\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\",\n",
      "    \"3\": \"LABEL_3\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2,\n",
      "    \"LABEL_3\": 3\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"problem_type\": \"single_label_classification\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.25.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30000\n",
      "}\n",
      "\n",
      "loading weights file classif/final_sublabel_model/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing BertForSequenceClassification.\n",
      "\n",
      "All the weights of BertForSequenceClassification were initialized from the model checkpoint at classif/final_sublabel_model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use BertForSequenceClassification for predictions without further training.\n",
      "The following columns in the test set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: nonfunction_Sentence, non_noun_adj, function_Sentence, non_noun, non_noun_verb_adj, non_verb, Sentence, non_verb_aux. If nonfunction_Sentence, non_noun_adj, function_Sentence, non_noun, non_noun_verb_adj, non_verb, Sentence, non_verb_aux are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Prediction *****\n",
      "  Num examples = 33\n",
      "  Batch size = 16\n",
      "/opt/conda/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default-505b13c249afb61c\n",
      "Found cached dataset csv (/home/jovyan/.cache/huggingface/datasets/csv/default-505b13c249afb61c/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f46c9d38a086483ba14115cd9a52dba4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /home/jovyan/.cache/huggingface/datasets/csv/default-505b13c249afb61c/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-fea5e4de696f89aa.arrow\n",
      "loading configuration file classif/final_sublabel_fuzzy_model_ooD/config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"classif/final_sublabel_fuzzy_model_ooD\",\n",
      "  \"architectures\": [\n",
      "    \"BertForSequenceClassification\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\",\n",
      "    \"3\": \"LABEL_3\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2,\n",
      "    \"LABEL_3\": 3\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"problem_type\": \"single_label_classification\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.25.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30000\n",
      "}\n",
      "\n",
      "loading weights file classif/final_sublabel_fuzzy_model_ooD/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing BertForSequenceClassification.\n",
      "\n",
      "All the weights of BertForSequenceClassification were initialized from the model checkpoint at classif/final_sublabel_fuzzy_model_ooD.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use BertForSequenceClassification for predictions without further training.\n",
      "The following columns in the test set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: nonfunction_Sentence, non_noun_adj, function_Sentence, non_noun, non_noun_verb_adj, non_verb, Sentence, non_verb_aux. If nonfunction_Sentence, non_noun_adj, function_Sentence, non_noun, non_noun_verb_adj, non_verb, Sentence, non_verb_aux are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Prediction *****\n",
      "  Num examples = 33\n",
      "  Batch size = 16\n",
      "/opt/conda/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "metrics = {}\n",
    "metrics[1] = {}\n",
    "\n",
    "forum_test = load_predict_testset(\"../Ablation_Studies/forum/sublabel/test_sublabel_preproc.csv\", model_path=\"classif/final_sublabel_model\")\n",
    "metrics[1][\"GloHBCD Orig - sublabel\"] = compute_test_metrics(forum_test, None)\n",
    "\n",
    "forum_test = load_predict_testset(\"../Ablation_Studies/forum/sublabel/test_sublabel_preproc.csv\", model_path=\"classif/final_sublabel_fuzzy_model_ooD\")\n",
    "metrics[1][\"GloHBCD Winner - sublabel\"] = compute_test_metrics(forum_test, None)\n",
    "\n",
    "forum_test = load_predict_testset(\"Nonsmoking_addition/ns_sublabel_test_set.csv\", model_path=\"classif/final_sublabel_model\")\n",
    "metrics[1][\"Nonsmoking Orig - sublabel\"] = compute_test_metrics(forum_test, None)\n",
    "\n",
    "forum_test = load_predict_testset(\"Nonsmoking_addition/ns_sublabel_test_set.csv\", model_path=\"classif/final_sublabel_fuzzy_model_ooD\")\n",
    "metrics[1][\"Nonsmoking Winner - sublabel\"] = compute_test_metrics(forum_test, None)\n",
    "\n",
    "forum_test = load_predict_testset(\"../Ablation_Studies/forum/sublabel/optifast_sublabel_preproc.csv\", model_path=\"classif/final_sublabel_model\")\n",
    "metrics[1][\"Optifast Orig - sublabel\"] = compute_test_metrics(forum_test, None)\n",
    "\n",
    "forum_test = load_predict_testset(\"../Ablation_Studies/forum/sublabel/optifast_sublabel_preproc.csv\", model_path=\"classif/final_sublabel_fuzzy_model_ooD\")\n",
    "metrics[1][\"Optifast Winner - sublabel\"] = compute_test_metrics(forum_test, None)\n",
    "\n",
    "forum_test = load_predict_testset(\"../Ablation_Studies/forum/sublabel/virtualcoach_sublabel_preproc.csv\", model_path=\"classif/final_sublabel_model\")\n",
    "metrics[1][\"Virtualcoach Orig - sublabel\"] = compute_test_metrics(forum_test, None)\n",
    "\n",
    "forum_test = load_predict_testset(\"../Ablation_Studies/forum/sublabel/virtualcoach_sublabel_preproc.csv\", model_path=\"classif/final_sublabel_fuzzy_model_ooD\")\n",
    "metrics[1][\"Virtualcoach Winner - sublabel\"] = compute_test_metrics(forum_test, None)\n",
    "\n",
    "forum_test = load_predict_testset(\"../Ablation_Studies/forum/sublabel/synthetic_sublabel_preproc.csv\", model_path=\"classif/final_sublabel_model\")\n",
    "metrics[1][\"Synthetic Orig - sublabel\"] = compute_test_metrics(forum_test, None)\n",
    "\n",
    "forum_test = load_predict_testset(\"../Ablation_Studies/forum/sublabel/synthetic_sublabel_preproc.csv\", model_path=\"classif/final_sublabel_fuzzy_model_ooD\")\n",
    "metrics[1][\"Synthetic Winner - sublabel\"] = compute_test_metrics(forum_test, None)\n",
    "\n",
    "forum_test = load_predict_testset(\"../Ablation_Studies/forum/sublabel/WoZ_sublabel_preproc.csv\", model_path=\"classif/final_sublabel_model\")\n",
    "metrics[1][\"WoZ Orig - sublabel\"] = compute_test_metrics(forum_test, None)\n",
    "\n",
    "forum_test = load_predict_testset(\"../Ablation_Studies/forum/sublabel/WoZ_sublabel_preproc.csv\", model_path=\"classif/final_sublabel_fuzzy_model_ooD\")\n",
    "metrics[1][\"WoZ Winner - sublabel\"] = compute_test_metrics(forum_test, None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_sublab = pd.DataFrame.from_dict({(i,j): metrics[i][j]\n",
    "                            for i in metrics.keys() \n",
    "                            for j in metrics[i].keys()}, \n",
    "                        orient='columns').transpose()\n",
    "\n",
    "#metrics_df = pd.concat([metrics_df, metrics_lab])\n",
    "#metrics_df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_sublab.to_csv(\"metrics/Sublabel_adding_new_data_test_Sets.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### checking winning test set label distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "valence = pd.read_csv(\"valence_predicted_enhanced_set.csv\")\n",
    "valence_orig = pd.read_csv(\"../Ablation_Studies/forum/valence/test_valence_preproc.csv\")\n",
    "valence_ns = pd.read_csv(\"../Ablation_Studies/forum/valence/nonsmoking_valence_preproc.csv\")\n",
    "label = pd.read_csv(\"label_predicted_enhanced_set.csv\")\n",
    "label_orig = pd.read_csv(\"../Ablation_Studies/forum/label/test_label_preproc.csv\")\n",
    "label_ns = pd.read_csv(\"../Ablation_Studies/forum/label/nonsmoking_label_preproc.csv\")\n",
    "sublabel = pd.read_csv(\"sublabel_predicted_enhanced_set.csv\")\n",
    "sublabel_orig = pd.read_csv(\"../Ablation_Studies/forum/sublabel/test_sublabel_preproc.csv\")\n",
    "sublabel_ns = pd.read_csv(\"../Ablation_Studies/forum/sublabel/nonsmoking_sublabel_preproc.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1    0.726368\n",
      "0    0.273632\n",
      "Name: labels, dtype: float64\n",
      "1    0.69697\n",
      "0    0.30303\n",
      "Name: labels, dtype: float64\n",
      "1    0.734139\n",
      "0    0.265861\n",
      "Name: labels, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "print(valence[\"labels\"].value_counts(normalize=True))\n",
    "print(valence_orig[\"labels\"].value_counts(normalize=True))\n",
    "print(valence_ns[\"labels\"].value_counts(normalize=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    0.731434\n",
      "1    0.188199\n",
      "2    0.080366\n",
      "Name: labels, dtype: float64\n",
      "0    0.648009\n",
      "1    0.256189\n",
      "2    0.095802\n",
      "Name: labels, dtype: float64\n",
      "0    0.658610\n",
      "1    0.232628\n",
      "2    0.108761\n",
      "Name: labels, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "print(label[\"labels\"].value_counts(normalize=True))\n",
    "print(label_orig[\"labels\"].value_counts(normalize=True))\n",
    "print(label_ns[\"labels\"].value_counts(normalize=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    0.800\n",
      "2    0.080\n",
      "1    0.076\n",
      "3    0.044\n",
      "Name: labels, dtype: float64\n",
      "0    0.691275\n",
      "1    0.154362\n",
      "2    0.093960\n",
      "3    0.060403\n",
      "Name: labels, dtype: float64\n",
      "0    0.704128\n",
      "2    0.149083\n",
      "1    0.100917\n",
      "3    0.045872\n",
      "Name: labels, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "print(sublabel[\"labels\"].value_counts(normalize=True))\n",
    "print(sublabel_orig[\"labels\"].value_counts(normalize=True))\n",
    "print(sublabel_ns[\"labels\"].value_counts(normalize=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Calculating amount of samples added per Confidence Threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_1 = load_dataset(\"csv\", data_files=\"mot_05.csv\", split=\"train\")\n",
    "dataset_2 = load_dataset(\"csv\", data_files=\"mot_07.csv\", split=\"train\")\n",
    "dataset_3 = load_dataset(\"csv\", data_files=\"mot_095.csv\", split=\"train\")\n",
    "dataset_4 = load_dataset(\"csv\", data_files=\"new_data_both.csv\", split=\"train\")\n",
    "dats = [dataset_1,dataset_2,dataset_3,dataset_4]\n",
    "\n",
    "FN = [0.5, 0.7, 0.95, 0]\n",
    "vals = [0.5, 0.7, 0.95]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.9/site-packages/transformers/pipelines/text_classification.py:89: UserWarning: `return_all_scores` is now deprecated, use `top_k=1` if you want similar functionnality\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "cond = []\n",
    "length = []\n",
    "label_num = 2\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"classif/final_valence_model\", num_labels=label_num, ignore_mismatched_sizes=True)\n",
    "pipe = TextClassificationPipeline(model=model, tokenizer=tokenizer, return_all_scores=True, device=0)\n",
    "for index, dat in enumerate(dats):\n",
    "    label_0_scores = []\n",
    "    label_1_scores = []\n",
    "    for out in pipe(KeyDataset(dat, \"split\"), batch_size=8, truncation=\"only_first\"):\n",
    "        label_0_scores.append(out[0]['score'])\n",
    "        label_1_scores.append(out[1]['score'])\n",
    "    pred_train_df = pd.DataFrame({\"split\": dat[:]['split'], \"0\":label_0_scores, \"1\": label_1_scores})\n",
    "    predict_1 = pd.concat([_1,a_1]).sample(frac=1)\n",
    "    for jindex, val in enumerate(vals):\n",
    "            _1 = pred_train_df[pred_train_df[\"0\"] > val].copy()\n",
    "            a_1 = pred_train_df[pred_train_df[\"1\"] > val].copy()\n",
    "            _1[\"labels\"] = 0\n",
    "            a_1[\"labels\"] = 1\n",
    "            predict_1 = pd.concat([_1,a_1]).sample(frac=1)\n",
    "            length.append(len(predict_1))\n",
    "            cond.append(str(FN[index]) + \" \" + str(val))    \n",
    "lengths = pd.DataFrame({'cond':cond, 'length':length})\n",
    "lengths.to_csv(\"metrics/val_count_datapoints.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.9/site-packages/transformers/pipelines/text_classification.py:89: UserWarning: `return_all_scores` is now deprecated, use `top_k=1` if you want similar functionnality\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "cond = []\n",
    "length = []\n",
    "label_num = 3\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"classif/final_label_model\", num_labels=label_num, ignore_mismatched_sizes=True)\n",
    "pipe = TextClassificationPipeline(model=model, tokenizer=tokenizer, return_all_scores=True, device=0)\n",
    "for index, dat in enumerate(dats):\n",
    "    label_0_scores = []\n",
    "    label_1_scores = []\n",
    "    label_2_scores = []\n",
    "    for out in pipe(KeyDataset(dat, \"split\"), batch_size=8, truncation=\"only_first\"):\n",
    "        label_0_scores.append(out[0]['score'])\n",
    "        label_1_scores.append(out[1]['score'])\n",
    "        label_2_scores.append(out[2]['score'])\n",
    "    pred_train_df = pd.DataFrame({\"split\": dat[:]['split'], \"0\":label_0_scores, \"1\": label_1_scores, \"2\": label_2_scores})\n",
    "    for jindex, val in enumerate(vals):\n",
    "            _1 = pred_train_df[pred_train_df[\"0\"] > val].copy()\n",
    "            a_1 = pred_train_df[pred_train_df[\"1\"] > val].copy()\n",
    "            d_1 = pred_train_df[pred_train_df[\"2\"] > val].copy()\n",
    "            _1[\"labels\"] = 0\n",
    "            a_1[\"labels\"] = 1\n",
    "            d_1[\"labels\"] = 2\n",
    "            predict_1 = pd.concat([_1,a_1, d_1]).sample(frac=1)\n",
    "            length.append(len(predict_1))\n",
    "            cond.append(str(FN[index]) + \" \" + str(val))    \n",
    "lengths = pd.DataFrame({'cond':cond, 'length':length})\n",
    "lengths.to_csv(\"metrics/label_count_datapoints.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.9/site-packages/transformers/pipelines/text_classification.py:89: UserWarning: `return_all_scores` is now deprecated, use `top_k=1` if you want similar functionnality\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "cond = []\n",
    "length = []\n",
    "label_num = 4\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"classif/final_sublabel_model\", num_labels=label_num, ignore_mismatched_sizes=True)\n",
    "pipe = TextClassificationPipeline(model=model, tokenizer=tokenizer, return_all_scores=True, device=0)\n",
    "for index, dat in enumerate(dats):\n",
    "    label_0_scores = []\n",
    "    label_1_scores = []\n",
    "    label_2_scores = []\n",
    "    label_3_scores = []\n",
    "    for out in pipe(KeyDataset(dat, \"split\"), batch_size=8, truncation=\"only_first\"):\n",
    "        label_0_scores.append(out[0]['score'])\n",
    "        label_1_scores.append(out[1]['score'])\n",
    "        label_2_scores.append(out[2]['score'])\n",
    "        label_3_scores.append(out[3]['score'])\n",
    "    pred_train_df = pd.DataFrame({\"split\": dat[:]['split'], \"0\":label_0_scores, \"1\": label_1_scores, \"2\":label_2_scores, \"3\": label_3_scores})\n",
    "    for jindex, val in enumerate(vals):\n",
    "            _1 = pred_train_df[pred_train_df[\"0\"] > val].copy()\n",
    "            a_1 = pred_train_df[pred_train_df[\"1\"] > val].copy()\n",
    "            d_1 = pred_train_df[pred_train_df[\"2\"] > val].copy()\n",
    "            n_1 = pred_train_df[pred_train_df[\"3\"] > val].copy()\n",
    "            _1[\"labels\"] = 0\n",
    "            a_1[\"labels\"] = 1\n",
    "            d_1[\"labels\"] = 2\n",
    "            n_1[\"labels\"] = 3\n",
    "            predict_1 = pd.concat([_1,a_1, d_1, n_1]).sample(frac=1)\n",
    "            length.append(len(predict_1))\n",
    "            cond.append(str(FN[index]) + \" \" + str(val))    \n",
    "lengths = pd.DataFrame({'cond':cond, 'length':length})\n",
    "lengths.to_csv(\"metrics/sublabel_count_datapoints.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3333, 12)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics = {}\n",
    "    for i in range(1):\n",
    "        print(i)\n",
    "        model = AutoModelForSequenceClassification.from_pretrained(\"bert-base-german-cased\", num_labels=label_num)\n",
    "        trainer = Trainer(model=model, args=training_args, train_dataset=forum_trains[i], eval_dataset=forum_vals[i], compute_metrics=compute_metrics)\n",
    "        trainer.train()\n",
    "        metrics[i] = trainer.evaluate()\n",
    "        dataset_1 = load_dataset(\"csv\", data_files=\"mot_05.csv\", split=\"train\")\n",
    "        dataset_2 = load_dataset(\"csv\", data_files=\"mot_07.csv\", split=\"train\")\n",
    "        dataset_3 = load_dataset(\"csv\", data_files=\"mot_095.csv\", split=\"train\")\n",
    "        dataset_4 = load_dataset(\"csv\", data_files=\"new_data_both.csv\", split=\"train\")\n",
    "        pipe = TextClassificationPipeline(model=model, tokenizer=tokenizer, return_all_scores=True, device=0)\n",
    "        label_0_scores = []\n",
    "        label_1_scores = []\n",
    "        #label_2_scores = []\n",
    "        #label_3_scores = []\n",
    "        for out in pipe(KeyDataset(dataset_1, \"split\"), batch_size=8, truncation=\"only_first\"):\n",
    "            label_0_scores.append(out[0]['score'])\n",
    "            label_1_scores.append(out[1]['score'])\n",
    "           # label_2_scores.append(out[2]['score'])\n",
    "           # label_3_scores.append(out[3]['score'])\n",
    "        pred_train_df_1 = pd.DataFrame({\"split\": dataset_1[:]['split'], \"0\":label_0_scores, \"1\": label_1_scores#, \"2\": label_2_scores, \"3\": label_3_scores \n",
    "                                       })\n",
    "        label_0_scores = []\n",
    "        label_1_scores = []\n",
    "       # label_2_scores = []\n",
    "       # label_3_scores = []\n",
    "        for out in pipe(KeyDataset(dataset_2, \"split\"), batch_size=8, truncation=\"only_first\"):\n",
    "            label_0_scores.append(out[0]['score'])\n",
    "            label_1_scores.append(out[1]['score'])\n",
    "            #label_2_scores.append(out[2]['score'])\n",
    "            #label_3_scores.append(out[3]['score'])\n",
    "        pred_train_df_2 = pd.DataFrame({\"split\": dataset_2[:]['split'], \"0\":label_0_scores, \"1\": label_1_scores#, \"2\": label_2_scores, \"3\": label_3_scores \n",
    "                                       })\n",
    "        label_0_scores = []\n",
    "        label_1_scores = []\n",
    "        #label_2_scores = []\n",
    "        #label_3_scores = []\n",
    "        for out in pipe(KeyDataset(dataset_3, \"split\"), batch_size=8, truncation=\"only_first\"):\n",
    "            label_0_scores.append(out[0]['score'])\n",
    "            label_1_scores.append(out[1]['score'])\n",
    "           # label_2_scores.append(out[2]['score'])\n",
    "           # label_3_scores.append(out[3]['score'])\n",
    "        pred_train_df_3 = pd.DataFrame({\"split\": dataset_3[:]['split'], \"0\":label_0_scores, \"1\": label_1_scores#, \"2\": label_2_scores, \"3\": label_3_scores  \n",
    "                                       })\n",
    "        '''label_0_scores = []\n",
    "        label_1_scores = []\n",
    "        label_2_scores = []\n",
    "        label_3_scores = []\n",
    "        for out in pipe(KeyDataset(dataset_4, \"split\"), batch_size=8, truncation=\"only_first\"):\n",
    "            label_0_scores.append(out[0]['score'])\n",
    "            label_1_scores.append(out[1]['score'])\n",
    "            label_2_scores.append(out[2]['score'])\n",
    "            label_3_scores.append(out[3]['score'])\n",
    "        pred_train_df_4 = pd.DataFrame({\"split\": dataset_4[:]['split'], \"0\":label_0_scores, \"1\": label_1_scores, \"2\": label_2_scores, \"3\": label_3_scores  })'''\n",
    "        for conf_score in conf_scores:\n",
    "            print(conf_score)\n",
    "            _1 = pred_train_df_1[pred_train_df_1[\"0\"] > conf_score].copy()\n",
    "            a_1 = pred_train_df_1[pred_train_df_1[\"1\"] > conf_score].copy()\n",
    "            #d_1 = pred_train_df_1[pred_train_df_1[\"2\"] > conf_score].copy()\n",
    "            #n_1 = pred_train_df_1[pred_train_df_1[\"3\"] > conf_score].copy()\n",
    "            _1[\"labels\"] = 0\n",
    "            a_1[\"labels\"] = 1\n",
    "           # d_1[\"labels\"] = 2\n",
    "           # n_1[\"labels\"] = 3\n",
    "            predict_1 = pd.concat([_1,a_1#, d_1, n_1\n",
    "                                  ]).sample(frac=1)\n",
    "            predict_1.rename(columns={\"split\":\"Sentence\"}, inplace=True)\n",
    "            predict_1.to_csv(\"enhanced_test_1.csv\")\n",
    "            enhanced_pred_1 = load_tokenize_enhanced_set(\"enhanced_test_1.csv\")\n",
    "            enhanced_1 = concatenate_datasets([forum_trains[i], enhanced_pred_1])\n",
    "            enhanced_1 = enhanced_1.shuffle()\n",
    "            model = AutoModelForSequenceClassification.from_pretrained(\"bert-base-german-cased\", num_labels=label_num)\n",
    "            trainer = Trainer(model=model, args=training_args, train_dataset=enhanced_1, eval_dataset=forum_vals[i], compute_metrics=compute_metrics)\n",
    "            trainer.train()\n",
    "            metrics[i][\"FN 05 \" + str(conf_score)] = trainer.evaluate()\n",
    "            trainer.save_model(\"classif/valence_FN_05_\"+str(conf_score)+\"_model\")\n",
    "            _2 = pred_train_df_2[pred_train_df_2[\"0\"] > conf_score].copy()\n",
    "            a_2 = pred_train_df_2[pred_train_df_2[\"1\"] > conf_score].copy()\n",
    "            #d_2 = pred_train_df_2[pred_train_df_2[\"2\"] > conf_score].copy()\n",
    "            #n_2 = pred_train_df_2[pred_train_df_2[\"3\"] > conf_score].copy()\n",
    "            _2[\"labels\"] = 0\n",
    "            a_2[\"labels\"] = 1\n",
    "           # d_2[\"labels\"] = 2\n",
    "           # n_2[\"labels\"] = 3\n",
    "            predict_2 = pd.concat([_2,a_2#, d_2, n_2\n",
    "                                  ]).sample(frac=1)\n",
    "            predict_2.rename(columns={\"split\":\"Sentence\"}, inplace=True)\n",
    "            predict_2.to_csv(\"enhanced_test_2.csv\")\n",
    "            enhanced_pred_2 = load_tokenize_enhanced_set(\"enhanced_test_2.csv\")\n",
    "            enhanced_2 = concatenate_datasets([forum_trains[i], enhanced_pred_2])\n",
    "            enhanced_2 = enhanced_2.shuffle()\n",
    "            model = AutoModelForSequenceClassification.from_pretrained(\"bert-base-german-cased\", num_labels=label_num)\n",
    "            trainer = Trainer(model=model, args=training_args, train_dataset=enhanced_2, eval_dataset=forum_vals[i], compute_metrics=compute_metrics)\n",
    "            trainer.train()\n",
    "            metrics[i][\"FN 07 \" + str(conf_score)] = trainer.evaluate()\n",
    "            trainer.save_model(\"classif/valence_FN_07_\"+str(conf_score)+\"_model\")\n",
    "            _3 = pred_train_df_3[pred_train_df_3[\"0\"] > conf_score].copy()\n",
    "            a_3 = pred_train_df_3[pred_train_df_3[\"1\"] > conf_score].copy()\n",
    "           # d_3 = pred_train_df_3[pred_train_df_3[\"2\"] > conf_score].copy()\n",
    "           # n_3 = pred_train_df_3[pred_train_df_3[\"3\"] > conf_score].copy()\n",
    "            _3[\"labels\"] = 0\n",
    "            a_3[\"labels\"] = 1\n",
    "           # d_3[\"labels\"] = 2\n",
    "           # n_3[\"labels\"] = 3\n",
    "            predict_3 = pd.concat([_3,a_3#, d_3, n_3\n",
    "                                  ]).sample(frac=1)\n",
    "            predict_3.rename(columns={\"split\":\"Sentence\"}, inplace=True)\n",
    "            predict_3.to_csv(\"enhanced_test_3.csv\")\n",
    "            enhanced_pred_3 = load_tokenize_enhanced_set(\"enhanced_test_3.csv\")\n",
    "            enhanced_3 = concatenate_datasets([forum_trains[i], enhanced_pred_3])\n",
    "            enhanced_3 = enhanced_3.shuffle()\n",
    "            model = AutoModelForSequenceClassification.from_pretrained(\"bert-base-german-cased\", num_labels=label_num)\n",
    "            trainer = Trainer(model=model, args=training_args, train_dataset=enhanced_3, eval_dataset=forum_vals[i], compute_metrics=compute_metrics)\n",
    "            trainer.train()\n",
    "            metrics[i][\"FN 095 \" + str(conf_score)] = trainer.evaluate()\n",
    "            '''_4 = pred_train_df_4[pred_train_df_4[\"0\"] > conf_score].copy()\n",
    "            a_4 = pred_train_df_4[pred_train_df_4[\"1\"] > conf_score].copy()\n",
    "            d_4 = pred_train_df_4[pred_train_df_4[\"2\"] > conf_score].copy()\n",
    "            n_4 = pred_train_df_4[pred_train_df_4[\"3\"] > conf_score].copy()\n",
    "            _4[\"labels\"] = 0\n",
    "            a_4[\"labels\"] = 1\n",
    "            d_4[\"labels\"] = 2\n",
    "            n_4[\"labels\"] = 3\n",
    "            predict_4 = pd.concat([_4,a_4, d_4, n_4]).sample(frac=1)\n",
    "            predict_4.rename(columns={\"split\":\"Sentence\"}, inplace=True)\n",
    "            predict_4.to_csv(\"enhanced_test_4.csv\")\n",
    "            enhanced_pred_4 = load_tokenize_enhanced_set(\"enhanced_test_4.csv\")\n",
    "            enhanced_4 = concatenate_datasets([forum_trains[i], enhanced_pred_4])\n",
    "            enhanced_4 = enhanced_4.shuffle()\n",
    "            model = AutoModelForSequenceClassification.from_pretrained(\"bert-base-german-cased\", num_labels=label_num)\n",
    "            trainer = Trainer(model=model, args=training_args, train_dataset=enhanced_4, eval_dataset=forum_vals[i], compute_metrics=compute_metrics)\n",
    "            trainer.train()\n",
    "            metrics[i][\"no FN \" + str(conf_score)] = trainer.evaluate()'''\n",
    "            trainer.save_model(\"classif/valence_FN_095_\"+str(conf_score)+\"_model\")\n",
    "            metrics_df = pd.DataFrame.from_dict({(i,j): metrics[i][j]\n",
    "                               for i in metrics.keys() \n",
    "                               for j in metrics[i].keys()},\n",
    "                           orient='columns').transpose()\n",
    "        #metrics_df.head()\n",
    "            metrics_df.to_csv(\"metrics/valence_FN_mot_test_sets_metrics_average_none.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(992, 4)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict.shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
